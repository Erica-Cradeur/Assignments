{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jCzuSRebutu"
      },
      "source": [
        "tar zxvf fr-en.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfLTBGxNbw6e"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OF1RB761bxWM"
      },
      "source": [
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSxepbR9bxY8"
      },
      "source": [
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "\tlengths = [len(s.split()) for s in sentences]\n",
        "\treturn min(lengths), max(lengths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT589GD1bxbL"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')\n",
        "\n",
        "# shortest and longest sentence lengths\n",
        "def sentence_lengths(sentences):\n",
        "\tlengths = [len(s.split()) for s in sentences]\n",
        "\treturn min(lengths), max(lengths)\n",
        "\n",
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
        "\n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "minlen, maxlen = sentence_lengths(sentences)\n",
        "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZZ8ueDvbxdm"
      },
      "source": [
        "English data: sentences=2007723, min=0, max=668\n",
        "French data: sentences=2007723, min=0, max=693"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BXrHqOWbxgQ"
      },
      "source": [
        "# clean a list of lines\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# normalize unicode characters\n",
        "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# remove non-printable chars form each token\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# remove tokens with numbers in them\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\treturn cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PQEP3gGbxix"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_sentences(doc):\n",
        "\treturn doc.strip().split('\\n')\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_lines(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor line in lines:\n",
        "\t\t# normalize unicode characters\n",
        "\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\tline = line.decode('UTF-8')\n",
        "\t\t# tokenize on white space\n",
        "\t\tline = line.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tline = [word.lower() for word in line]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tline = [word.translate(table) for word in line]\n",
        "\t\t# remove non-printable chars form each token\n",
        "\t\tline = [re_print.sub('', w) for w in line]\n",
        "\t\t# remove tokens with numbers in them\n",
        "\t\tline = [word for word in line if word.isalpha()]\n",
        "\t\t# store as string\n",
        "\t\tcleaned.append(' '.join(line))\n",
        "\treturn cleaned\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load English data\n",
        "filename = 'europarl-v7.fr-en.en'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'english.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])\n",
        "\n",
        "# load French data\n",
        "filename = 'europarl-v7.fr-en.fr'\n",
        "doc = load_doc(filename)\n",
        "sentences = to_sentences(doc)\n",
        "sentences = clean_lines(sentences)\n",
        "save_clean_sentences(sentences, 'french.pkl')\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vesreX8Gbxl2"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Z5_xJZb-OW"
      },
      "source": [
        "# create a frequency table for all words\n",
        "def to_vocab(lines):\n",
        "\tvocab = Counter()\n",
        "\tfor line in lines:\n",
        "\t\ttokens = line.split()\n",
        "\t\tvocab.update(tokens)\n",
        "\treturn vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noatqglbb-RV"
      },
      "source": [
        "# remove all words with a frequency below a threshold\n",
        "def trim_vocab(vocab, min_occurance):\n",
        "\ttokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "\treturn set(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMxLdqpHb-VA"
      },
      "source": [
        "# mark all OOV with \"unk\" for all lines\n",
        "def update_dataset(lines, vocab):\n",
        "\tnew_lines = list()\n",
        "\tfor line in lines:\n",
        "\t\tnew_tokens = list()\n",
        "\t\tfor token in line.split():\n",
        "\t\t\tif token in vocab:\n",
        "\t\t\t\tnew_tokens.append(token)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_tokens.append('unk')\n",
        "\t\tnew_line = ' '.join(new_tokens)\n",
        "\t\tnew_lines.append(new_line)\n",
        "\treturn new_lines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEo-q3eIcC3w"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from collections import Counter\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_sentences(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# create a frequency table for all words\n",
        "def to_vocab(lines):\n",
        "\tvocab = Counter()\n",
        "\tfor line in lines:\n",
        "\t\ttokens = line.split()\n",
        "\t\tvocab.update(tokens)\n",
        "\treturn vocab\n",
        "\n",
        "# remove all words with a frequency below a threshold\n",
        "def trim_vocab(vocab, min_occurance):\n",
        "\ttokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
        "\treturn set(tokens)\n",
        "\n",
        "# mark all OOV with \"unk\" for all lines\n",
        "def update_dataset(lines, vocab):\n",
        "\tnew_lines = list()\n",
        "\tfor line in lines:\n",
        "\t\tnew_tokens = list()\n",
        "\t\tfor token in line.split():\n",
        "\t\t\tif token in vocab:\n",
        "\t\t\t\tnew_tokens.append(token)\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_tokens.append('unk')\n",
        "\t\tnew_line = ' '.join(new_tokens)\n",
        "\t\tnew_lines.append(new_line)\n",
        "\treturn new_lines\n",
        "\n",
        "# load English dataset\n",
        "filename = 'english.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('English Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New English Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'english_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(lines[i])\n",
        "\n",
        "# load French dataset\n",
        "filename = 'french.pkl'\n",
        "lines = load_clean_sentences(filename)\n",
        "# calculate vocabulary\n",
        "vocab = to_vocab(lines)\n",
        "print('French Vocabulary: %d' % len(vocab))\n",
        "# reduce vocabulary\n",
        "vocab = trim_vocab(vocab, 5)\n",
        "print('New French Vocabulary: %d' % len(vocab))\n",
        "# mark out of vocabulary words\n",
        "lines = update_dataset(lines, vocab)\n",
        "# save updated dataset\n",
        "filename = 'french_vocab.pkl'\n",
        "save_clean_sentences(lines, filename)\n",
        "# spot check\n",
        "for i in range(10):\n",
        "\tprint(lines[i])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}