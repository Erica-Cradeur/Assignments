{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3Doom.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXURty4R4a-8",
        "outputId": "d321ae55-01f1-42f6-88a8-ad449c4435c2"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential zlib1g-dev libsdl2-dev libjpeg-dev nasm tar libbz2-dev libgtk2.0-dev cmake git libfluidsynth-dev libgme-dev libopenal-dev timidity libwildmidi-dev unzip\n",
        "\n",
        "# Boost libraries\n",
        "!sudo apt-get install libboost-all-dev\n",
        "\n",
        "# Lua binding dependencies\n",
        "!apt-get install liblua5.1-dev\n",
        "!sudo apt-get install cmake libboost-all-dev libgtk2.0-dev libsdl2-dev python-numpy git\n",
        "!git clone https://github.com/shakenes/vizdoomgym.git\n",
        "!python3 -m pip install -e vizdoomgym/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "^C\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "libgtk2.0-dev is already the newest version (2.24.32-1ubuntu1).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libfluidsynth-dev is already the newest version (1.1.9-1).\n",
            "libgme-dev is already the newest version (0.6.2-1).\n",
            "libopenal-dev is already the newest version (1:1.18.2-2).\n",
            "libwildmidi-dev is already the newest version (0.4.2-1).\n",
            "nasm is already the newest version (2.13.02-0.1).\n",
            "timidity is already the newest version (2.13.2-41).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.9).\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "tar is already the newest version (1.29b-2ubuntu0.2).\n",
            "unzip is already the newest version (6.0-21ubuntu1.1).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Reading package lists... Done\n",
            "^C\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'liblua5.1-0-dev' instead of 'liblua5.1-dev'\n",
            "liblua5.1-0-dev is already the newest version (5.1.5-8.1build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgtk2.0-dev is already the newest version (2.24.32-1ubuntu1).\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "git is already the newest version (1:2.17.1-1ubuntu0.9).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "fatal: destination path 'vizdoomgym' already exists and is not an empty directory.\n",
            "Obtaining file:///content/vizdoomgym\n",
            "Collecting vizdoom@ https://github.com/mwydmuch/ViZDoom/tarball/1.1.8pre\n",
            "  Using cached https://github.com/mwydmuch/ViZDoom/tarball/1.1.8pre\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from vizdoomgym==1.0) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vizdoomgym==1.0) (1.19.5)\n",
            "Collecting pre-commit\n",
            "  Using cached pre_commit-2.15.0-py2.py3-none-any.whl (191 kB)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->vizdoomgym==1.0) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->vizdoomgym==1.0) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->vizdoomgym==1.0) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->vizdoomgym==1.0) (0.16.0)\n",
            "Collecting virtualenv>=20.0.8\n",
            "  Using cached virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from pre-commit->vizdoomgym==1.0) (0.10.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Collecting cfgv>=2.0.0\n",
            "  Using cached cfgv-3.3.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting nodeenv>=0.11.1\n",
            "  Using cached nodeenv-1.6.0-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from pre-commit->vizdoomgym==1.0) (4.8.2)\n",
            "Collecting identify>=1.0.0\n",
            "  Using cached identify-2.4.0-py2.py3-none-any.whl (98 kB)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Using cached platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting backports.entry-points-selectable>=1.0.4\n",
            "  Using cached backports.entry_points_selectable-1.1.1-py2.py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->vizdoomgym==1.0) (3.4.0)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.8->pre-commit->vizdoomgym==1.0) (1.15.0)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Using cached distlib-0.3.3-py2.py3-none-any.whl (496 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit->vizdoomgym==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->pre-commit->vizdoomgym==1.0) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OdNW6gq5xJ0"
      },
      "source": [
        "#Debug to check environment works\n",
        "import gym\n",
        "import vizdoomgym\n",
        "!\n",
        "env = gym.make('VizdoomHealthGathering-v0')\n",
        "\n",
        "# use like a normal Gym environment\n",
        "state = env.reset()\n",
        "state, reward, done, info = env.step(env.action_space.sample())\n",
        "print(state.shape)\n",
        "# env.render()\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZeYoNpZ7Cy3"
      },
      "source": [
        "#TF2 removed support for contrib layers, so we need to reinstall TF for 1.15 from 2.2.0\n",
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JTvrx_G7C0Y"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers import flatten, conv2d, fully_connected\n",
        "from collections import deque, Counter\n",
        "import random\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWAvdp_27C3u"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixa60UO47C7Z"
      },
      "source": [
        "from skimage.color import rgb2gray\n",
        "from skimage import transform\n",
        "\n",
        "#prepro (210, 160, 3) uint8 frame into 30x40 1D float vector \n",
        "color = np.array([240, 320, 74]).mean()\n",
        "def preprocess_observation(obs):\n",
        "    \n",
        "    \n",
        "    img =obs/255.0\n",
        "    img[img==color] = 0\n",
        "\n",
        "    img_gray = rgb2gray(img)\n",
        "    preprocessed_frame = transform.resize(img_gray, [60,80])\n",
        "    #From (240, 320, 3) to (60,80,1), reducing further seriously reduces clarity and contrast (30,40,1)\n",
        "    return preprocessed_frame\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukE_Zgcr7C99"
      },
      "source": [
        "env = gym.make('VizdoomHealthGathering-v0')\n",
        "n_outputs = env.action_space.n\n",
        "print(n_outputs)\n",
        "\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "for i in range(22):\n",
        "  \n",
        "  if i > 20:\n",
        "    print(observation.shape)\n",
        "    plt.imshow(observation)\n",
        "    plt.show()\n",
        "\n",
        "  observation, _, _, _ = env.step(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtIhZBAY7DAV"
      },
      "source": [
        "#Take a look at the preprocessed inputs in greyscale\n",
        "#Let's compare the original and preprocessed tensors.\n",
        "\n",
        "obs_preprocessed = preprocess_observation(observation)\n",
        "plt.imshow(obs_preprocessed)\n",
        "plt.show()\n",
        "print(observation.shape)\n",
        "print(obs_preprocessed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH7yovp47Nnp"
      },
      "source": [
        "#Frame Stacking, used to supply another minibatch within a minibatch, in order to judge motion. orginally in deepmind paper\n",
        "#Basically, as long as we always use stacks for all steps, and are consistent, there will not be any significant gaps in frequency\n",
        "\n",
        "\"\"\"\n",
        "We have all these frames, but what gets fed as input to the “Q-Network”? Paper used\n",
        " a sequence of four game frames stacked together, making the data dimension (4,84,84). \n",
        " The idea is that the action agents choose depends on the prior sequence of game frames. \n",
        " Imagine playing Breakout, for instance. Is the ball moving up or down? \n",
        " If the ball is moving down, you better get the paddle in position to bounce it back up. \n",
        " If the thatball is moving up, you can wait a little longer or try to move in the opposite direction as needed if you think the ball will eventually reach there.\n",
        "\n",
        "Due to the way that Atari renders screens, every other frame may not aactually be rendered.\n",
        "This is negatively affecting our performance, so instead, we take Deepmind's approach of element wise maxima\n",
        "Create stack of 4, with 2 images combined via elementwise-maxima\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "stack_size = 4 # We stack 4 composite frames in total\n",
        "\n",
        "# Initialize deque with zero-images one array for each image. Deque is a special kind of queue that deletes last entry when new entry comes in\n",
        "stacked_frames  =  deque([np.zeros((60,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "\n",
        "def stack_frames(stacked_frames, state, is_new_episode):\n",
        "    # Preprocess frame\n",
        "    frame = preprocess_observation(state)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stacked_frames\n",
        "        stacked_frames = deque([np.zeros((60,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
        "        \n",
        "        # Because we're in a new episode, copy the same frame 4x, apply elementwise maxima\n",
        "        maxframe = np.maximum(frame,frame)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        stacked_frames.append(maxframe)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Stack the frames\n",
        "        stacked_state = np.stack(stacked_frames, axis=2)\n",
        "        \n",
        "    else:\n",
        "        #Since deque append adds t right, we can fetch rightmost element\n",
        "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stacked_frames.append(maxframe)\n",
        "\n",
        "        # Build the stacked state (first dimension specifies different frames)\n",
        "        stacked_state = np.stack(stacked_frames, axis=2) \n",
        "    \n",
        "    return stacked_state, stacked_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms4qUhZu7NqT"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "#Reset is technically not necessary if variables done  in TF2\n",
        "#https://github.com/ageron/tf2_course/issues/8\n",
        "\n",
        "def q_network(X, name_scope):\n",
        "    \n",
        "    # Initialize layers\n",
        "    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n",
        "\n",
        "    with tf.compat.v1.variable_scope(name_scope) as scope: \n",
        "\n",
        "\n",
        "        # initialize the convolutional layers\n",
        "        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n",
        "        tf.compat.v1.summary.histogram('layer_1',layer_1)\n",
        "        \n",
        "        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_2',layer_2)\n",
        "        \n",
        "        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('layer_3',layer_3)\n",
        "        \n",
        "        # Flatten the result of layer_3 before feeding to the fully connected layer\n",
        "        flat = flatten(layer_3)\n",
        "        # Insert fully connected layer\n",
        "        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('fc',fc)\n",
        "        #Add final output layer\n",
        "        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n",
        "        tf.compat.v1.summary.histogram('output',output)\n",
        "        \n",
        "\n",
        "        # Vars will store the parameters of the network such as weights\n",
        "        vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n",
        "        #Return both variables and outputs together\n",
        "        return vars, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvkpYjBu7UiW"
      },
      "source": [
        "epsilon = 0.5\n",
        "eps_min = 0.05\n",
        "eps_max = 1.0\n",
        "eps_decay_steps = 500000\n",
        "\n",
        "#\n",
        "def epsilon_greedy(action, step):\n",
        "    p = np.random.random(1).squeeze() #1D entries returned using squeeze\n",
        "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) #Decaying policy with more steps\n",
        "    if p< epsilon:\n",
        "        return np.random.randint(n_outputs)\n",
        "    else:\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I97G5Ofx7WrC"
      },
      "source": [
        "buffer_len = 20000\n",
        "#Buffer is made from a deque - double ended queue\n",
        "exp_buffer = deque(maxlen=buffer_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjeGhB8W7Wsr"
      },
      "source": [
        "def sample_memories(batch_size):\n",
        "    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n",
        "    mem = np.array(exp_buffer)[perm_batch]\n",
        "    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm5uWXwz7WvN"
      },
      "source": [
        "num_episodes = 1000\n",
        "batch_size = 48\n",
        "\n",
        "input_shape = (None, 60, 80, 1)\n",
        "\n",
        "learning_rate = 0.002\n",
        "#Modified for composite stacked frames\n",
        "X_shape = (None, 60, 80, 4)\n",
        "discount_factor = 0.99\n",
        "\n",
        "global_step = 0\n",
        "copy_steps = 100\n",
        "steps_train = 4\n",
        "start_steps = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ZITKY27Wxl"
      },
      "source": [
        "logdir = 'logs'\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# Now we define the placeholder for our input i.e game state\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n",
        "\n",
        "# we define a boolean called in_training_model to toggle the training\n",
        "in_training_mode = tf.compat.v1.placeholder(tf.bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYkwaw6Z7W0I"
      },
      "source": [
        "# we build our Q network, which takes the input X and generates Q values for all the actions in the state\n",
        "mainQ, mainQ_outputs = q_network(X, 'mainQ')\n",
        "\n",
        "# similarly we build our target Q network\n",
        "targetQ, targetQ_outputs = q_network(X, 'targetQ')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ImpWZfG7fBf"
      },
      "source": [
        "# define the placeholder for our action values\n",
        "\n",
        "X_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\n",
        "Q_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vvH4tCC7fDe"
      },
      "source": [
        "# define a placeholder for our output i.e action\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n",
        "\n",
        "# now we calculate the loss which is the difference between actual value and predicted value\n",
        "loss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n",
        "\n",
        "# we use adam optimizer for minimizing the loss\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "loss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\n",
        "merge_summary = tf.compat.v1.summary.merge_all()\n",
        "file_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsQ6DG2-7fFu"
      },
      "source": [
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    \n",
        "    # for each episode\n",
        "    history = []\n",
        "    for i in range(5):\n",
        "        done = False\n",
        "        obs = env.reset()\n",
        "        epoch = 0\n",
        "        episodic_reward = 0\n",
        "        actions_counter = Counter() \n",
        "        episodic_loss = []\n",
        "        #First step, begin stacking frames\n",
        "        \n",
        "        obs,stacked_frames= stack_frames(stacked_frames,obs,True)\n",
        "\n",
        "        # while the state is not the terminal state\n",
        "        while not done:\n",
        "\n",
        "           #Data generation using the untrained network\n",
        "        \n",
        "            \n",
        "\n",
        "            # feed the game screen and get the Q values for each action,  FEED THE NETWORK BY CALLING THE OUTPUT LAYER\n",
        "            \n",
        "            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n",
        "\n",
        "            # get the action\n",
        "            action = np.argmax(actions, axis=-1)\n",
        "            actions_counter[str(action)] += 1 \n",
        "\n",
        "            # select the action using epsilon greedy policy\n",
        "            action = epsilon_greedy(action, global_step)\n",
        "            \n",
        "            # now perform the action and move to the next state, next_obs, receive reward\n",
        "            next_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "            #Begin stacking intra-episode code\n",
        "            next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
        "\n",
        "            # Store this transistion as an experience in the replay buffer! Quite important\n",
        "            exp_buffer.append([obs, action, next_obs, reward, done])\n",
        "            \n",
        "            # After certain steps, we train our Q network with samples from the experience replay buffer\n",
        "            if global_step % steps_train == 0 and global_step > start_steps:\n",
        "                #Our buffer should already contain everything preprocessed and stacked\n",
        "                # sample experience, mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n",
        "                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n",
        "\n",
        "                # states\n",
        "                o_obs = [x for x in o_obs]\n",
        "\n",
        "                # next states\n",
        "                o_next_obs = [x for x in o_next_obs]\n",
        "\n",
        "                # next actions\n",
        "                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n",
        "\n",
        "\n",
        "                # discounted reward: these are our Y-values\n",
        "                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n",
        "\n",
        "                # merge all summaries and write to the file\n",
        "                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n",
        "                file_writer.add_summary(mrg_summary, global_step)\n",
        "\n",
        "                # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n",
        "                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n",
        "                episodic_loss.append(train_loss)\n",
        "            \n",
        "            # after some interval we copy our main Q network weights to target Q network\n",
        "            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n",
        "                copy_target_to_main.run()\n",
        "                \n",
        "            obs = next_obs\n",
        "            epoch += 1\n",
        "            global_step += 1\n",
        "            episodic_reward += reward\n",
        "\n",
        "        next_obs=np.zeros(obs.shape)\n",
        "        exp_buffer.append([obs, action, next_obs, reward, done])\n",
        "        obs= env.reset()\n",
        "        obs,stacked_frames= stack_frames(stacked_frames,obs,True) \n",
        "        \n",
        "        history.append(episodic_reward)\n",
        "        print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", len(history))\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHPGeSwK7fHy"
      },
      "source": [
        "plt.plot(history)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6-ByrLU7fJz"
      },
      "source": [
        "#Visualization cobe for running within Colab\n",
        "\n",
        "# Install dependencies first for graphics visualization within Colaboratory\n",
        "\n",
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install ffmpeg\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6rHIo-s7w-L"
      },
      "source": [
        "#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n",
        "\n",
        "\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTmNNNul7xA0"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('vid/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './vid',video_callable=lambda episode_id:True, force=True)\n",
        "  return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlvCouu375Ck"
      },
      "source": [
        "#Evaluate model on openAi GYM\n",
        "import gym\n",
        "env = gym.make('VizdoomHealthGathering-v0')\n",
        "\n",
        "environment = wrap_env(env)\n",
        "done = False\n",
        "observation = environment.reset()\n",
        "new_observation = observation\n",
        "\n",
        "prev_input = None\n",
        "img_array=[]\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    init.run()\n",
        "    observation, stacked_frames = stack_frames(stacked_frames, observation, True)\n",
        "    \n",
        "    while True:\n",
        "       \n",
        "    \n",
        "        #set input to network to be difference image\n",
        "  \n",
        "        \n",
        "        #print(observation.shape)\n",
        "        \n",
        "        \n",
        "\n",
        "        # feed the game screen and get the Q values for each action\n",
        "        actions = mainQ_outputs.eval(feed_dict={X:[observation], in_training_mode:False})\n",
        "\n",
        "        # get the action\n",
        "        action = np.argmax(actions, axis=-1)\n",
        "        actions_counter[str(action)] += 1 \n",
        "\n",
        "        # select the action using epsilon greedy policy\n",
        "        action = epsilon_greedy(action, global_step)\n",
        "        environment.render()\n",
        "        new_observation, stacked_frames = stack_frames(stacked_frames, new_observation, False)\n",
        "        \n",
        "        observation = new_observation        \n",
        "        # now perform the action and move to the next state, next_obs, receive reward\n",
        "        new_observation, reward, done, _ = environment.step(action)\n",
        "        \n",
        "        img_array.append(new_observation)\n",
        "        if done: \n",
        "          #observation = env.reset()\n",
        "          break\n",
        "      \n",
        "    environment.close()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv8_2_c_77qg"
      },
      "source": [
        "\"\"\"\n",
        "Works fine\n",
        "\"\"\"\n",
        "\n",
        "for i in range(10):\n",
        "    print(img_array[i].shape)\n",
        "    plt.imshow(img_array[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10TusJLA7-ee"
      },
      "source": [
        "print(len(img_array))\n",
        "!pip install sk-video "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13LGnKUb7-_p"
      },
      "source": [
        "from random import choice\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import numpy as np\n",
        "import skvideo.io\n",
        "\n",
        "out_video =  np.empty([len(img_array), 240, 320, 3], dtype = np.uint8)\n",
        "out_video =  out_video.astype(np.uint8)\n",
        "\n",
        "for i in range(len(img_array)):\n",
        "  frame = img_array[i]\n",
        "  out_video[i] = frame\n",
        "# Writes the the output image sequences in a video file\n",
        "skvideo.io.vwrite(\"/content/doom.mp4\", out_video)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}